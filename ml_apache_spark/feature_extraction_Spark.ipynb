{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import rand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZER \n",
    "\n",
    "### A tokenizer is used to break a sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (1, \"Spark is a distributed computing system.\"),\n",
    "    (2, \"It provides interfaces for multiple languages\"),\n",
    "    (3, \"Spark is built on top of Hadoop\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+\n",
      "|id |sentence                                     |\n",
      "+---+---------------------------------------------+\n",
      "|1  |Spark is a distributed computing system.     |\n",
      "|2  |It provides interfaces for multiple languages|\n",
      "|3  |Spark is built on top of Hadoop              |\n",
      "+---+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol = 'sentence', outputCol = 'wods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = tokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|id |sentence                                     |wods                                                |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|1  |Spark is a distributed computing system.     |[spark, is, a, distributed, computing, system.]     |\n",
      "|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|\n",
      "|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "\n",
    "### countVectorizer is used to convert text into numerical format. It gives the count of each word in a given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+\n",
      "|id |words                                            |\n",
      "+---+-------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|\n",
      "+---+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n",
    "            (2, \"I love Python Spark supports Python\".split()),\n",
    "            (3, \"Spark solves the big problem of big data\".split())]\n",
    "\n",
    "textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n",
    "\n",
    "textdata.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol = 'words', outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+----------------------------------------------------+\n",
      "|id |words                                            |features                                            |\n",
      "+---+-------------------------------------------------+----------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,3,5,7],[2.0,1.0,1.0,1.0,1.0,1.0])        |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,3,12],[1.0,2.0,1.0,1.0,1.0])             |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,4,6,8,9,10,11],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+---+-------------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "### Term Frequency - Inverse Document Frequency is used to quantify the importante of a word in a document. \n",
    "\n",
    "### TF-IDF is computed by multiplying the number of times a word occurs in a document by the inverse document frequency of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "|id |sentence             |\n",
      "+---+---------------------+\n",
      "|1  |Spark supports python|\n",
      "|2  |Spark is fast        |\n",
      "|3  |Spark is easy        |\n",
      "+---+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "        (1, \"Spark supports python\"),\n",
    "        (2, \"Spark is fast\"),\n",
    "        (3, \"Spark is easy\")\n",
    "    ], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-------------------------+\n",
      "|id |sentence             |words                    |\n",
      "+---+---------------------+-------------------------+\n",
      "|1  |Spark supports python|[spark, supports, python]|\n",
      "|2  |Spark is fast        |[spark, is, fast]        |\n",
      "|3  |Spark is easy        |[spark, is, easy]        |\n",
      "+---+---------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol = 'sentence', outputCol = 'words')\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-------------------------+--------------------------+\n",
      "|id |sentence             |words                    |rawFeatures               |\n",
      "+---+---------------------+-------------------------+--------------------------+\n",
      "|1  |Spark supports python|[spark, supports, python]|(10,[4,6,9],[1.0,1.0,1.0])|\n",
      "|2  |Spark is fast        |[spark, is, fast]        |(10,[3,6,9],[1.0,1.0,1.0])|\n",
      "|3  |Spark is easy        |[spark, is, easy]        |(10,[0,6,9],[1.0,1.0,1.0])|\n",
      "+---+---------------------+-------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol = 'words', outputCol = 'rawFeatures', numFeatures = 10)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an IDF object \n",
    "# mention the rawFeatures column as input \n",
    "# mention the features column as output\n",
    "\n",
    "idf = IDF(inputCol = 'rawFeatures', outputCol = 'features')\n",
    "idfMdodel = idf.fit(featurizedData)\n",
    "tfidData = idfMdodel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------------------+\n",
      "|sentence             |features                                 |\n",
      "+---------------------+-----------------------------------------+\n",
      "|Spark supports python|(10,[4,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "|Spark is fast        |(10,[3,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "|Spark is easy        |(10,[0,6,9],[0.6931471805599453,0.0,0.0])|\n",
      "+---------------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidData.select(\"sentence\", \"features\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWordsRemover\n",
    "\n",
    "### StopWordsRemover is a transformer that filters out stop words like 'an', 'a', 'the'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData = spark.createDataFrame([\n",
    "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
    "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
    "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+\n",
      "|id |sentence                                                    |\n",
      "+---+------------------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |\n",
      "+---+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol = 'sentence', outputCol = 'filtered_sentence')\n",
    "textData = remover.transform(textData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|id |sentence                                                    |filtered_sentence                                   |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringIndexer\n",
    "\n",
    "### StringIndexer converts a column of strings into a column of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = spark.createDataFrame(\n",
    "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
    "    [\"id\", \"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| color|\n",
      "+---+------+\n",
      "|  0|   red|\n",
      "|  1|   red|\n",
      "|  2|  blue|\n",
      "|  3|yellow|\n",
      "|  4|yellow|\n",
      "|  5|yellow|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol = 'color', outputCol= 'colorIndex')\n",
    "indexed = indexer.fit(colors).transform(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id| color|colorIndex|\n",
      "+---+------+----------+\n",
      "|  0|   red|       1.0|\n",
      "|  1|   red|       1.0|\n",
      "|  2|  blue|       2.0|\n",
      "|  3|yellow|       0.0|\n",
      "|  4|yellow|       0.0|\n",
      "|  5|yellow|       0.0|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StandardScaler\n",
    "\n",
    "### StandardScaler transformed the data so that it has a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  1| [70.0,170.0,17.0]|\n",
      "|  2| [80.0,165.0,25.0]|\n",
      "|  3|[65.0,150.0,135.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(1, Vectors.dense([70, 170, 17])),\n",
    "        (2, Vectors.dense([80, 165, 25])),\n",
    "        (3, Vectors.dense([65, 150, 135]))]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------------------------------------+\n",
      "|id |features          |scaledFeatures                                             |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "|1  |[70.0,170.0,17.0] |[-0.218217890235993,0.8006407690254367,-0.6369487984517485]|\n",
      "|2  |[80.0,165.0,25.0] |[1.0910894511799611,0.3202563076101752,-0.5156252177942725]|\n",
      "|3  |[65.0,150.0,135.0]|[-0.8728715609439701,-1.120897076635609,1.152574016246021] |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol = 'features', outputCol = 'scaledFeatures', withStd = True, withMean = True)\n",
    "scalerModel = scaler.fit(df)\n",
    "scaledData = scalerModel.transform(df)\n",
    "scaledData.show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Exercise - Feature Extraction and Transformation using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData = spark.read.csv('proverbs.csv', header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------+\n",
      "|id |text                                                       |\n",
      "+---+-----------------------------------------------------------+\n",
      "|1  |When in Rome do as the Romans do.                          |\n",
      "|2  |Do not judge a book by its cover.                          |\n",
      "|3  |Actions speak louder than words.                           |\n",
      "|4  |A picture is worth a thousand words.                       |\n",
      "|5  |If at first you do not succeed try try again.              |\n",
      "|6  |Practice makes perfect.                                    |\n",
      "|7  |An apple a day keeps the doctor away.                      |\n",
      "|8  |When the going gets tough the tough get going.             |\n",
      "|9  |All is fair in love and war.                               |\n",
      "|10 |Too many cooks spoil the broth.                            |\n",
      "|11 |You can not make an omelette without breaking eggs.        |\n",
      "|12 |The early bird catches the worm.                           |\n",
      "|13 |Better late than never.                                    |\n",
      "|14 |Honesty is the best policy.                                |\n",
      "|15 |A penny saved is a penny earned.                           |\n",
      "|16 |Two wrongs do not make a right.                            |\n",
      "|17 |The grass is always greener on the other side of the fence.|\n",
      "|18 |Do not count your chickens before they're hatched.         |\n",
      "|19 |Laughter is the best medicine.                             |\n",
      "|20 |Rome wasn't built in a day.                                |\n",
      "+---+-----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data = spark.read.csv(\"mpg.csv\", header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "|15.0|        8|      390.0|       190|  3850|       8.5|  70|American|\n",
      "|21.0|        6|      199.0|        90|  2648|      15.0|  70|American|\n",
      "|18.0|        6|      199.0|        97|  2774|      15.5|  70|American|\n",
      "|16.0|        8|      304.0|       150|  3433|      12.0|  70|American|\n",
      "|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+--------------------------------------------------------+\n",
      "|id |text                                         |words                                                   |\n",
      "+---+---------------------------------------------+--------------------------------------------------------+\n",
      "|1  |When in Rome do as the Romans do.            |[when, in, rome, do, as, the, romans, do.]              |\n",
      "|2  |Do not judge a book by its cover.            |[do, not, judge, a, book, by, its, cover.]              |\n",
      "|3  |Actions speak louder than words.             |[actions, speak, louder, than, words.]                  |\n",
      "|4  |A picture is worth a thousand words.         |[a, picture, is, worth, a, thousand, words.]            |\n",
      "|5  |If at first you do not succeed try try again.|[if, at, first, you, do, not, succeed, try, try, again.]|\n",
      "+---+---------------------------------------------+--------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol = 'text', outputCol = 'words')\n",
    "textData = tokenizer.transform(textData)\n",
    "textData.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+--------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|id |text                                         |words                                                   |features                                                             |\n",
      "+---+---------------------------------------------+--------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|1  |When in Rome do as the Romans do.            |[when, in, rome, do, as, the, romans, do.]              |(99,[0,4,5,11,12,41,69,93],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])        |\n",
      "|2  |Do not judge a book by its cover.            |[do, not, judge, a, book, by, its, cover.]              |(99,[1,3,4,19,20,31,44,54],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])        |\n",
      "|3  |Actions speak louder than words.             |[actions, speak, louder, than, words.]                  |(99,[7,10,81,86,97],[1.0,1.0,1.0,1.0,1.0])                           |\n",
      "|4  |A picture is worth a thousand words.         |[a, picture, is, worth, a, thousand, words.]            |(99,[1,2,10,70,77,87],[2.0,1.0,1.0,1.0,1.0,1.0])                     |\n",
      "|5  |If at first you do not succeed try try again.|[if, at, first, you, do, not, succeed, try, try, again.]|(99,[3,4,16,17,22,35,53,62,64],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+---+---------------------------------------------+--------------------------------------------------------+---------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countVector = CountVectorizer(inputCol = 'words', outputCol = 'features')\n",
    "textData = countVector.fit(textData).transform(textData)\n",
    "textData.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "|MPG |Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|Origin  |OriginIndex|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "|15.0|8        |390.0      |190       |3850  |8.5       |70  |American|0.0        |\n",
      "|21.0|6        |199.0      |90        |2648  |15.0      |70  |American|0.0        |\n",
      "|18.0|6        |199.0      |97        |2774  |15.5      |70  |American|0.0        |\n",
      "|16.0|8        |304.0      |150       |3433  |12.0      |70  |American|0.0        |\n",
      "|14.0|8        |455.0      |225       |3086  |10.0      |70  |American|0.0        |\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringIndex = StringIndexer(inputCol = 'Origin', outputCol = 'OriginIndex')\n",
    "mpg_data = stringIndex.fit(mpg_data).transform(mpg_data)\n",
    "mpg_data.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|OriginIndex|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "|15.0|        8|      390.0|       190|  3850|       8.5|  70|American|        0.0|\n",
      "|21.0|        6|      199.0|        90|  2648|      15.0|  70|American|        0.0|\n",
      "|18.0|        6|      199.0|        97|  2774|      15.5|  70|American|        0.0|\n",
      "|16.0|        8|      304.0|       150|  3433|      12.0|  70|American|        0.0|\n",
      "|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|        0.0|\n",
      "|15.0|        8|      350.0|       165|  3693|      11.5|  70|American|        0.0|\n",
      "|18.0|        8|      307.0|       130|  3504|      12.0|  70|American|        0.0|\n",
      "|14.0|        8|      454.0|       220|  4354|       9.0|  70|American|        0.0|\n",
      "|15.0|        8|      400.0|       150|  3761|       9.5|  70|American|        0.0|\n",
      "|10.0|        8|      307.0|       200|  4376|      15.0|  70|American|        0.0|\n",
      "|15.0|        8|      383.0|       170|  3563|      10.0|  70|American|        0.0|\n",
      "|11.0|        8|      318.0|       210|  4382|      13.5|  70|American|        0.0|\n",
      "|10.0|        8|      360.0|       215|  4615|      14.0|  70|American|        0.0|\n",
      "|15.0|        8|      429.0|       198|  4341|      10.0|  70|American|        0.0|\n",
      "|21.0|        6|      200.0|        85|  2587|      16.0|  70|American|        0.0|\n",
      "|17.0|        8|      302.0|       140|  3449|      10.5|  70|American|        0.0|\n",
      "| 9.0|        8|      304.0|       193|  4732|      18.5|  70|American|        0.0|\n",
      "|14.0|        8|      340.0|       160|  3609|       8.0|  70|American|        0.0|\n",
      "|22.0|        6|      198.0|        95|  2833|      15.5|  70|American|        0.0|\n",
      "|14.0|        8|      440.0|       215|  4312|       8.5|  70|American|        0.0|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg_data.orderBy('OriginIndex').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+------------------------+\n",
      "|MPG |Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|Origin  |OriginIndex|features                |\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+------------------------+\n",
      "|15.0|8        |390.0      |190       |3850  |8.5       |70  |American|0.0        |[8.0,390.0,190.0,3850.0]|\n",
      "|21.0|6        |199.0      |90        |2648  |15.0      |70  |American|0.0        |[6.0,199.0,90.0,2648.0] |\n",
      "|18.0|6        |199.0      |97        |2774  |15.5      |70  |American|0.0        |[6.0,199.0,97.0,2774.0] |\n",
      "|16.0|8        |304.0      |150       |3433  |12.0      |70  |American|0.0        |[8.0,304.0,150.0,3433.0]|\n",
      "|14.0|8        |455.0      |225       |3086  |10.0      |70  |American|0.0        |[8.0,455.0,225.0,3086.0]|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['Cylinders', 'Engine Disp', 'Horsepower', 'Weight'], outputCol = 'features')\n",
    "mpg_transformed_data = assembler.transform(mpg_data)\n",
    "mpg_transformed_data.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol = 'features', outputCol = 'scaledFeature', withStd = True, withMean=True)\n",
    "scalerModel = scaler.fit(mpg_transformed_data)\n",
    "scaledData = scalerModel.transform(mpg_transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+------------------------+-----------------------------------------------------------------------------------+\n",
      "|MPG |Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|Origin  |OriginIndex|features                |scaledFeature                                                                      |\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+------------------------+-----------------------------------------------------------------------------------+\n",
      "|15.0|8        |390.0      |190       |3850  |8.5       |70  |American|0.0        |[8.0,390.0,190.0,3850.0]|[1.48205302652896,1.869079955831451,2.222084561602166,1.027093462353608]           |\n",
      "|21.0|6        |199.0      |90        |2648  |15.0      |70  |American|0.0        |[6.0,199.0,90.0,2648.0] |[0.3095711165403583,0.043843985634147174,-0.37591456792553746,-0.38801882543985255]|\n",
      "|18.0|6        |199.0      |97        |2774  |15.5      |70  |American|0.0        |[6.0,199.0,97.0,2774.0] |[0.3095711165403583,0.043843985634147174,-0.1940546288585982,-0.2396792678175763]  |\n",
      "|16.0|8        |304.0      |150       |3433  |12.0      |70  |American|0.0        |[8.0,304.0,150.0,3433.0]|[1.48205302652896,1.0472459587792617,1.1828849097910845,0.5361601645084557]        |\n",
      "|14.0|8        |455.0      |225       |3086  |10.0      |70  |American|0.0        |[8.0,455.0,225.0,3086.0]|[1.48205302652896,2.4902335582546176,3.131384256936862,0.12763773200901246]        |\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+------------------------+-----------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
